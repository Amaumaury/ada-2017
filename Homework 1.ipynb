{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1\"><a href=\"#Task-1.-Compiling-Ebola-Data\"><span class=\"toc-item-num\">Task 1.&nbsp;&nbsp;</span>Compiling Ebola Data</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-2.-RNA-Sequences\"><span class=\"toc-item-num\">Task 2.&nbsp;&nbsp;</span>RNA Sequences</a></div>\n",
    " <div class=\"lev1\"><a href=\"#Task-3.-Class-War-in-Titanic\"><span class=\"toc-item-num\">Task 3.&nbsp;&nbsp;</span>Class War in Titanic</a></div></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, os\n",
    "import random\n",
    "pd.options.mode.chained_assignment = None\n",
    "idx = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../Data' # Use the data folder provided in Tutorial 02 - Intro to Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1. Compiling Ebola Data\n",
    "\n",
    "The `DATA_FOLDER/ebola` folder contains summarized reports of Ebola cases from three countries (Guinea, Liberia and Sierra Leone) during the recent outbreak of the disease in West Africa. For each country, there are daily reports that contain various information about the outbreak in several cities in each country.\n",
    "\n",
    "Use pandas to import these data files into a single `Dataframe`.\n",
    "Using this `DataFrame`, calculate for *each country*, the *daily average per month* of *new cases* and *deaths*.\n",
    "Make sure you handle all the different expressions for *new cases* and *deaths* that are used in the reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to clean the data, we made the following assumptions : \n",
    "- A person, counted in one of the three categories (i.e. confirmed, suspect, probable) for deaths and cases, can't be counted in another category. This reasonnable assumption guarantees that we are not counting people multiple times when computing the means.\n",
    "- As told during the lab session, we consider the Totals/National field for each country as consistent with respect to the different region/cities reports\n",
    "- Cumulative description/variable (e.g. Total deaths of confirmed) are assumed to carry all items from previous days that has not been registered. More precisely, let d1 and d2 be two dates such that d1 < d2  and d2 - d1 > 1 (with respect to time). Then d2 contains information about the corresponding description from d1 + 1 to d2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data for each country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_country(country_name):\n",
    "    return pd.concat(map(pd.read_csv, glob.glob(r'' + DATA_FOLDER + '/ebola/' + country_name + '_data/*.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load the data into a dataframe for each country\n",
    "guinea_dataframe = load_country('guinea')\n",
    "liberia_dataframe = load_country('liberia')\n",
    "sierraleone_dataframe = load_country('sl')\n",
    "\n",
    "#Print columns label\n",
    "print('Guinea:', guinea_dataframe.columns.get_values())\n",
    "print('Liberia:', liberia_dataframe.columns.get_values())\n",
    "print('Sierra Leone:', sierraleone_dataframe.columns.get_values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the interesting descriptions/variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "guinea_descriptions = set(guinea_dataframe.Description.values.tolist())\n",
    "liberia_descriptions = set(liberia_dataframe.Variable.values.tolist())\n",
    "sierraleone_descriptions = set(sierraleone_dataframe.variable.values.tolist())\n",
    "\n",
    "print(len(guinea_descriptions))\n",
    "print(len(liberia_descriptions))\n",
    "print(len(sierraleone_descriptions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# All possible descriptions for guinea\n",
    "print(guinea_descriptions)\n",
    "\n",
    "# From which we decided to keep only the following descriptions\n",
    "filtered_guinea_descriptions = ['New cases of confirmed', \n",
    "                                'New cases of suspects', \n",
    "                                'New cases of probables',\n",
    "                                'Total deaths of confirmed',\n",
    "                                'Total deaths of suspects',\n",
    "                                'Total deaths of probables']\n",
    "\n",
    "# Rename dictionary that will be useful later\n",
    "guinea_rename_dict = {'New cases of confirmed': 'cases_confirmed',\n",
    "                      'New cases of suspects': 'cases_suspected',\n",
    "                      'New cases of probables': 'cases_probable',\n",
    "                      'Total deaths of confirmed': 'deaths_confirmed',\n",
    "                      'Total deaths of suspects': 'deaths_suspected',\n",
    "                      'Total deaths of probables': 'deaths_probable'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For guinea, the descriptions 'New deaths registered today (probables)' and 'New deaths registered today (suspects)' or not really interesting as there are only available for one day and equal to 0. Moreover the description 'New deaths registered today (confirmed)' is also available during only one day and is a redundancy of the description 'New deaths registered today'. Thus we decided to drop those descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# All possible descriptions for liberia\n",
    "print(liberia_descriptions)\n",
    "\n",
    "# From which we decided to keep only the following descriptions\n",
    "filtered_liberia_descriptions = ['New case/s (confirmed)', \n",
    "                                 'New Case/s (Suspected)', \n",
    "                                 'New Case/s (Probable)',\n",
    "                                 'Total death/s in confirmed cases',\n",
    "                                 'Total death/s in suspected cases',\n",
    "                                 'Total death/s in probable cases']\n",
    "\n",
    "liberia_rename_dict = {'New case/s (confirmed)': 'cases_confirmed',\n",
    "                       'New Case/s (Suspected)': 'cases_suspected',\n",
    "                       'New Case/s (Probable)': 'cases_probable',\n",
    "                       'Total death/s in confirmed cases': 'deaths_confirmed',\n",
    "                       'Total death/s in suspected cases': 'deaths_suspected',\n",
    "                       'Total death/s in probable cases': 'deaths_probable'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# All possible descriptions for Sierra Leone\n",
    "print(sierraleone_descriptions)\n",
    "\n",
    "# From which we decided to keep only the following descriptions\n",
    "filtered_sierraleone_descriptions = ['death_confirmed', \n",
    "                                     'death_probable', \n",
    "                                     'death_suspected',\n",
    "                                     'new_confirmed', \n",
    "                                     'new_probable', \n",
    "                                     'new_suspected']\n",
    "\n",
    "sierraleone_rename_dict = {'death_confirmed': 'deaths_confirmed',\n",
    "                           'death_suspected': 'deaths_suspected',\n",
    "                           'death_probable': 'deaths_probable',\n",
    "                           'new_confirmed': 'cases_confirmed',\n",
    "                           'new_probable': 'cases_probable',\n",
    "                           'new_suspected': 'cases_suspected'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "print('Guinea has duplicates ?', True in guinea_dataframe.duplicated(subset=['Date', 'Description']).values)\n",
    "print('Liberia has duplicates ?', True in liberia_dataframe.duplicated(subset=['Date', 'Variable']).values)\n",
    "print('Sierra Leone has duplicates ?', True in sierraleone_dataframe.duplicated(subset=['date', 'variable']).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "liberia_dataframe = liberia_dataframe.drop_duplicates(['Date', 'Variable'])\n",
    "sierraleone_dataframe = sierraleone_dataframe.drop_duplicates(['date', 'variable'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep only interesting columns\n",
    "guinea_dataframe = guinea_dataframe[['Date', 'Description', 'Totals']]\n",
    "liberia_dataframe = liberia_dataframe[['Date', 'Variable', 'National']]\n",
    "sierraleone_dataframe = sierraleone_dataframe[['date', 'variable', 'National']]\n",
    "\n",
    "# Standardize the date field for each dataframe\n",
    "guinea_dataframe.Date = pd.to_datetime(guinea_dataframe.Date)\n",
    "liberia_dataframe.Date = pd.to_datetime(liberia_dataframe.Date)\n",
    "sierraleone_dataframe.date = pd.to_datetime(sierraleone_dataframe.date)\n",
    "\n",
    "# Keep only the insteresting variables/descriptions\n",
    "guinea_dataframe = guinea_dataframe[[des in filtered_guinea_descriptions for des in guinea_dataframe.Description]]\n",
    "liberia_dataframe = liberia_dataframe[[var in filtered_liberia_descriptions for var in liberia_dataframe.Variable]]\n",
    "sierraleone_dataframe = sierraleone_dataframe[[var in filtered_sierraleone_descriptions for var in sierraleone_dataframe.variable]]\n",
    "\n",
    "# Remove rows with missing value\n",
    "guinea_dataframe = guinea_dataframe.dropna()\n",
    "liberia_dataframe = liberia_dataframe.dropna()\n",
    "sierraleone_dataframe = sierraleone_dataframe.dropna()\n",
    "\n",
    "# Cast all values to int\n",
    "guinea_dataframe.Totals = guinea_dataframe.Totals.astype(int)\n",
    "liberia_dataframe.National = liberia_dataframe.National.astype(int)\n",
    "sierraleone_dataframe.National = sierraleone_dataframe.National.astype(int)\n",
    "\n",
    "# Rename all reports (i.e. variable/description)\n",
    "guinea_dataframe.Description = guinea_dataframe.Description.apply(lambda des: guinea_rename_dict[des]) \n",
    "liberia_dataframe.Variable = liberia_dataframe.Variable.apply(lambda var: liberia_rename_dict[var])\n",
    "sierraleone_dataframe.variable = sierraleone_dataframe.variable.apply(lambda var: sierraleone_rename_dict[var])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Guinea dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the guinea dataframe, we noticed that the original 'Total deaths...' descriptions were cumulative values. We thus had to get the daily new deaths cases for each day. This is what is done in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_cum_guinea(des_name, df):\n",
    "    r_df = df.copy()\n",
    "    guinea_parts_to_clean = r_df[r_df.Description == des_name]\n",
    "    guinea_parts_to_clean = guinea_parts_to_clean.set_index(['Date', 'Description']).sort_index()\n",
    "    \n",
    "    cleaned_guinea_parts = (guinea_parts_to_clean - guinea_parts_to_clean.shift(1))\n",
    "    \n",
    "    idx = pd.IndexSlice\n",
    "    r_df = r_df.set_index(['Date', 'Description']).sort_index()\n",
    "    r_df.loc[idx[:, [des_name]], :] = cleaned_guinea_parts\n",
    "    r_df = r_df.dropna().reset_index()\n",
    "    return r_df\n",
    "    \n",
    "guinea_dataframe = clean_cum_guinea('deaths_confirmed', guinea_dataframe)\n",
    "guinea_dataframe = clean_cum_guinea('deaths_suspected', guinea_dataframe)\n",
    "guinea_dataframe = clean_cum_guinea('deaths_probable', guinea_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice from the guinea dataframe below that **some reports yield negative values**. Unless they were giving unexpected average values, we generaly decided to interpret them as corrections on previous reports and thus keep them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "guinea_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Liberia dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Last record for in liberia_new_deaths_... is equal to 0 which is a dirty value as it should be greater than the value in the precedent record\n",
    "# '...' here means {'confirmed', 'probable', 'suspected'}\n",
    "\n",
    "liberia_dataframe = liberia_dataframe.set_index(['Date', 'Variable']).sort_index()\n",
    "\n",
    "index0 = liberia_dataframe.loc[idx[:, 'deaths_confirmed'], :][-1:].index\n",
    "index1 = liberia_dataframe.loc[idx[:, 'deaths_probable'], :][-1:].index\n",
    "index2 = liberia_dataframe.loc[idx[:, 'deaths_suspected'], :][-1:].index\n",
    "\n",
    "liberia_dataframe = liberia_dataframe.drop(index0)\n",
    "liberia_dataframe = liberia_dataframe.drop(index1)\n",
    "liberia_dataframe = liberia_dataframe.drop(index2)\n",
    "\n",
    "liberia_dataframe = liberia_dataframe.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, from the liberia datafame below, that some rows at the end are dirty as they switch to a cumulative fashion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "liberia_dataframe[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell deals with the cumulative values found at the end of the dataframe for certain reports\n",
    "\n",
    "dirty_rows_liberia = liberia_dataframe[liberia_dataframe.Date > pd.datetime(2014, 12, 2)]\n",
    "dirty_rows_liberia = dirty_rows_liberia.set_index(['Date', 'Variable']).sort_index()\n",
    "\n",
    "cleaned_rows_liberia = dirty_rows_liberia - dirty_rows_liberia.shift(3)\n",
    "\n",
    "liberia_dataframe = liberia_dataframe.set_index(['Date', 'Variable']).sort_index()\n",
    "liberia_dataframe.loc[pd.datetime(2014, 12, 4):, :] = cleaned_rows_liberia\n",
    "liberia_dataframe = liberia_dataframe.dropna().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell deals with cumulative records (similar to the guinea cleaning case)\n",
    "\n",
    "def clean_cum_liberia(des_name, df):\n",
    "    r_df = df.copy()\n",
    "    liberia_parts_to_clean = r_df[r_df.Variable == des_name]\n",
    "    liberia_parts_to_clean = liberia_parts_to_clean.set_index(['Date', 'Variable']).sort_index()\n",
    "    \n",
    "    cleaned_liberia_parts = (liberia_parts_to_clean - liberia_parts_to_clean.shift(1))\n",
    "    \n",
    "    idx = pd.IndexSlice\n",
    "    r_df = r_df.set_index(['Date', 'Variable']).sort_index()\n",
    "    r_df.loc[idx[:, [des_name]], :] = cleaned_liberia_parts\n",
    "    r_df = r_df.dropna().reset_index()\n",
    "    return r_df\n",
    "\n",
    "liberia_dataframe = clean_cum_liberia('deaths_confirmed', liberia_dataframe)\n",
    "liberia_dataframe = clean_cum_liberia('deaths_suspected', liberia_dataframe)\n",
    "liberia_dataframe = clean_cum_liberia('deaths_probable', liberia_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Sierra Leone dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell deals with cumulative values (similar to guinea and liberia cases)\n",
    "\n",
    "sierraleone_dataframe_copy = sierraleone_dataframe.copy().set_index(['date', 'variable']).unstack()\n",
    "tmp = sierraleone_dataframe.set_index(['date', 'variable']).unstack()\n",
    "tmp = tmp.National[['deaths_confirmed', 'deaths_probable', 'deaths_suspected']]\n",
    "cleaned = (tmp - tmp.shift(1)).fillna(value=0)\n",
    "\n",
    "sierraleone_dataframe_copy.National.loc[1:, ['deaths_confirmed', 'deaths_probable', 'deaths_suspected']] = cleaned[1:]\n",
    "sierraleone_dataframe = sierraleone_dataframe_copy.stack().reset_index()[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove first three lines \n",
    "sierraleone_dataframe = sierraleone_dataframe[3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation and merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keeps only the year and month of each report\n",
    "guinea_dataframe['Period'] = guinea_dataframe.Date.dt.to_period('M')\n",
    "liberia_dataframe['Period'] = liberia_dataframe.Date.dt.to_period('M')\n",
    "sierraleone_dataframe['Period'] = sierraleone_dataframe.date.dt.to_period('M')\n",
    "\n",
    "# Keep only the day number of the month. It will be necessary when computing the means\n",
    "guinea_dataframe.Date = guinea_dataframe.Date.apply(lambda x: x.day)\n",
    "liberia_dataframe.Date = liberia_dataframe.Date.apply(lambda x: x.day)\n",
    "sierraleone_dataframe.date = sierraleone_dataframe.date.apply(lambda x: x.day)\n",
    "\n",
    "# Add a country column to each dataframe. This is required to merge the 3 different dataframes.\n",
    "guinea_dataframe['Country'] = 'Guinea'\n",
    "liberia_dataframe['Country'] = 'Liberia'\n",
    "sierraleone_dataframe['Country'] = 'Sierraleone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute averages for each month\n",
    "averages_guinea_dataframe = pd.DataFrame(guinea_dataframe.groupby(['Country', 'Period', 'Description']).apply(lambda group: max(group['Totals'].sum() / group['Date'].max(), 0)))\n",
    "averages_liberia_dataframe = pd.DataFrame(liberia_dataframe.groupby(['Country', 'Period', 'Variable']).apply(lambda group: group['National'].sum() / group['Date'].max()))\n",
    "averages_sierraleone_dataframe = pd.DataFrame(sierraleone_dataframe.groupby(['Country', 'Period', 'variable']).apply(lambda group: group['National'].sum() / group['date'].max()))\n",
    "\n",
    "# Remove index\n",
    "averages_guinea_dataframe = averages_guinea_dataframe.reset_index()\n",
    "averages_liberia_dataframe = averages_liberia_dataframe.reset_index()\n",
    "averages_sierraleone_dataframe = averages_sierraleone_dataframe.reset_index()\n",
    "\n",
    "# Rename columns\n",
    "averages_guinea_dataframe.columns = ['country', 'date', 'report', 'average']\n",
    "averages_liberia_dataframe.columns = ['country', 'date', 'report', 'average']\n",
    "averages_sierraleone_dataframe.columns = ['country', 'date', 'report', 'average']\n",
    "\n",
    "# Stack all averages into one dataframe\n",
    "all_averages_dataframe = pd.concat([averages_guinea_dataframe, averages_liberia_dataframe, averages_sierraleone_dataframe])\n",
    "\n",
    "# Sort final dataframe, set index and unstack result (for readibility reasons)\n",
    "all_averages_dataframe = all_averages_dataframe.sort_values(['date']).set_index(['country', 'date', 'report']).unstack()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_averages_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. RNA Sequences\n",
    "\n",
    "In the `DATA_FOLDER/microbiome` subdirectory, there are 9 spreadsheets of microbiome data that was acquired from high-throughput RNA sequencing procedures, along with a 10<sup>th</sup> file that describes the content of each. \n",
    "\n",
    "Use pandas to import the first 9 spreadsheets into a single `DataFrame`.\n",
    "Then, add the metadata information from the 10<sup>th</sup> spreadsheet as columns in the combined `DataFrame`.\n",
    "Make sure that the final `DataFrame` has a unique index and all the `NaN` values have been replaced by the tag `unknown`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f423518d22f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Extract metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_FOLDER\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/microbiome/metadata.xls'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mMIDs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mMID_combined\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#Extract metadata\n",
    "metadata = pd.read_excel(DATA_FOLDER + '/microbiome/metadata.xls')\n",
    "\n",
    "MIDs={};            \n",
    "MID_combined={};\n",
    "\n",
    "# load MID1-MID9 files\n",
    "for i in range(1,10):\n",
    "    # parse the DataFrame with metavalues\n",
    "    group_str = str(metadata[metadata.BARCODE == ('MID'+str(i))]['GROUP'].values[0]); #e.g. NEC1\n",
    "    sample_str = str(metadata[metadata.BARCODE == ('MID'+str(i))]['SAMPLE'].values[0]); #stool or tissue\n",
    "    if(sample_str == 'nan'):\n",
    "        sample_str='unknown';\n",
    "    # prase excel with column values 'NAME' and sample_str\n",
    "    MIDs[i-1] = pd.read_excel(DATA_FOLDER+'/microbiome/MID'+str(i)+'.xls', sheetname='Sheet 1', header=None, names=['NAME',sample_str])\n",
    "    #add 1 more level with 'group_str'\n",
    "    MIDs[i-1].columns = pd.MultiIndex.from_arrays([MIDs[i-1].columns, ['',group_str ]])\n",
    "\n",
    "    \n",
    "# Merge the DataFrames together\n",
    "MID_combined = MIDs[0];\n",
    "for i in range(1,9):\n",
    "    MID_combined = pd.DataFrame.merge(MID_combined, MIDs[i], how='outer')\n",
    "\n",
    "# Setting all NaN to 'unknown' label\n",
    "MID_combined = MID_combined.fillna('unknown')\n",
    "MID_combined.set_index('NAME', inplace=True)\n",
    "\n",
    "# Checking if the Index is unique\n",
    "print('Is the \"NAME\" index unique ? : ' + str(MID_combined.index.is_unique))\n",
    "\n",
    "# Printing some values\n",
    "MID_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Class War in Titanic\n",
    "\n",
    "Use pandas to import the data file `Data/titanic.xls`. It contains data on all the passengers that travelled on the Titanic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 <span style=\"color:Green\"> Importing the Needed Libraries </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "import pandas as pd\n",
    "DATA_FOLDER = \"Data\"\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_excel('Data/titanic.xls', sheetname='titanic', header=0)\n",
    "HTML(filename=DATA_FOLDER+'/titanic.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Raw DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the following questions state clearly your assumptions and discuss your findings:\n",
    "1. Describe the *type* and the *value range* of each attribute. Indicate and transform the attributes that can be `Categorical`. \n",
    "2. Plot histograms for the *travel class*, *embarkation port*, *sex* and *age* attributes. For the latter one, use *discrete decade intervals*. \n",
    "3. Calculate the proportion of passengers by *cabin floor*. Present your results in a *pie chart*.\n",
    "4. For each *travel class*, calculate the proportion of the passengers that survived. Present your results in *pie charts*.\n",
    "5. Calculate the proportion of the passengers that survived by *travel class* and *sex*. Present your results in *a single histogram*.\n",
    "6. Create 2 equally populated *age categories* and calculate survival proportions by *age category*, *travel class* and *sex*. Present your results in a `DataFrame` with unique index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1<span style=\"color:Green\"> Describe the type and the value range of each attribute. Indicate and transform the attributes that can be Categorical.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *pclass*: The type is integer since, as indicated, there are three types of pclasses namely: 1, 2, and 3. Thus the range is from 1 to 3 with discrete integerers. It can be set to categorical.\n",
    "\n",
    "- *survived*: The type is also integer as the passenger can either survive ('True') which corresponds to 1 or die ('False') which corresponds to 0. Can be categorical.\n",
    "\n",
    "- *name*: String since these are the names of the passengers. Can't be categorical due to the fact that there are many possibilities to what the name can be.\n",
    "\n",
    "- *sex*: A string referring to the sex of the passenger and thus having only two possibilities; \"Male\" or \"Female\".\n",
    "\n",
    "- *age*: Double or float since one's age is continuous and not discrete. The range is from 0 to the maximum age of a passenger (will assume 100). As it's continuous, it's not categorical.\n",
    "\n",
    "- *slbsp*: Since it represents the number of siblings, it should be an integer as it only can indicate full numbers. It ranges from 0 to max number of siblings a person has onboard. In this case, it's 8. Can be categorical, but it becomes a bit repetitive and thus should be kept non categorical. \n",
    "\n",
    "- *parch*: Since it represents the number of parents and children, it should be an integer as it only can indicate full numbers. It can range from 0 to max number of children a parent can have onboard. In this case, it's 9. Also kept non-categorical.\n",
    "\n",
    "- *ticket*: The type is object since it represents a string as ticket stands for ticket number. Since not all the ticket numbers are actual integers and some contain characters, the type is string and not integer.Non-categorical.\n",
    "\n",
    "- *fare*: This is the fare of the ticket in the British Pound currency. It's a float number and ranges from 0 to a high number. Since it's continuous, it can't be categorical.\n",
    "\n",
    "- *cabin*: The type of this attribute is String since it contains the cabin number along with the floor. \n",
    "    - *cabin floor*: The letter indicating the floor number. It is categorical.\n",
    "    \n",
    "- *embarked*: This attribute refers to a String representing the embarcation location with 3 values.: C, Q, S. In the spreadsheet, you can't tell what these values stand for. However, taking a look at the HTML file, we can see that these stand for Cherbourg, Queenstown, and Southhampton, respectively. Thus, it can be categorical. \n",
    "\n",
    "- *boat*: This attribute is an integer. Noticing that whenever the survived value for a passenger is 0 which means he hasn't survived, the boat attribute takes a NAN value. This means that the attribute refers to the boat id the survived passenger took. Ranges from???????\n",
    "\n",
    "- *body*: A float numerical refering to the body id number of the dead passenger. Its range might be from 1 to the number of dead passengers/identified dead bodies. \n",
    "\n",
    "- *home.dest*: String indicating the home and final destination of the passenger. Not categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data and Transforming Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, before even trying to manipulate and plot the data, we should make sure that the data is clean: the claims that were made previously are true. Moreover, we should know what should be done with any missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*pclass:* We have no missing data. We also approve that there are only three categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The unique categories should be: {1,2,3}\")\n",
    "print('The actual unique categories in the data given are: {}'.format(df['pclass'].unique()))\n",
    "print(\"Number of missing data for 'pclass' attribute: \" + str(np.sum(pd.isnull(df['pclass']))))\n",
    "df[\"pclass\"] = df[\"pclass\"].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*survived:* Again, we have no missing data. We also approve that there are only two categories. This attribute can be transformed into categorical, but we won't do it since we need it as non-categorical when we want to calculate the proportion of the passengers that survived by travel class and sex. This is because the factor plot inputs must contain one non-categorical input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The unique categories should be: {0,1}\")\n",
    "print('The actual unique categories in the data given are: {}'.format(df['survived'].unique()))\n",
    "print(\"Number of missing data for 'survived' attribute: \" + str(np.sum(pd.isnull(df['survived']))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*sex:* No missing data. Also, all the values are either \"Male\" or \"Female\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The unique categories should be: {\\\"Male\\\",\\\"Female\\\"}\")\n",
    "print('The actual unique categories in the data given are: {}'.format(df['sex'].unique()))\n",
    "print(\"Number of missing data for 'sex' attribute: \" + str(np.sum(pd.isnull(df['sex']))))\n",
    "df[\"sex\"] = df[\"sex\"].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*age:* We have 263 NaN values (missing data) for passenger ages. We also calculate the range of the numerical attribute. There are 12 age values less than 1, therefore we know that it us not an error. Additionally, we apply a (to-month) transformation to them and we obtain values between 1 and 11 included. This assures that the values represent month ages. In order not to create an additional DATAFRAME, we will keep the NaN values untouched for the time being. Later on, when we want to plot, we can drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of missing data for 'age' attribute: \" + str(np.sum(pd.isnull(df['age']))))\n",
    "print(\"Range of the 'age' attribute: [\" + str(df['age'].min()) +', ' +  str(df['age'].max()) + \"] years.\" )\n",
    "# We would also like to check whether there are a significant number of ages less than 1 to see whether \n",
    "#it's an error or if it represents a baby of age less than 1.\n",
    "print(\"Number of ages less than 1: \" + str(len([x for x in df['age'] if x<1])) + \" occurences.\")\n",
    "print(\"The below values are in years:\\n \" + str(df[df['age']<1]['age']) + \" \\nWhile the ones below are in months: \\n\" + str(np.round(df[df['age']<1]['age']*12)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*sibsp* and *parch*: We calculate the range of these values and check how many missing entries there are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of missing data for 'sibsp' attribute: \" + str(np.sum(pd.isnull(df['sibsp']))))\n",
    "print(\"Range of the 'sibsp' attribute: [\" + str(df['sibsp'].min()) +', ' +  str(df['sibsp'].max()) + \"] persons.\" )\n",
    "print(\"Number of missing data for 'parch' attribute: \" + str(np.sum(pd.isnull(df['parch']))))\n",
    "print(\"Range of the 'sibsp' attribute: [\" + str(df['parch'].min()) +', ' +  str(df['parch'].max()) + \"] persons.\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*cabins* and *floors:* The cabin number is made up of a letter followed by a number. Logically, the first letter represents the floor in the ship, and the number represents the room in that specific floor. In order to associate the data against the attribute *floor*, we have to create the new attribute *floors*, or at least try and see its possible values.  Also, we keep the NaN values untouched at the moment, and will treat them later before plotting. We have to note that there are 1014 missing data entries for cabin, so the data left is not very representative but might provide us with some insight. \n",
    "\n",
    "Additionally, we can observe from the HTML file that some cabin entries start with two letters such as *F E46*. Since these are categorized along with the other F values, we assume that this is a cabin located in floor F but also extending to floor E. This is probably because it's a duplex suite. We will disregard the fact that it extends to two floors and assume that the floor corresponding to such cabins is the first letter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['floors'] = np.NaN # Created a new column in our dataframe called 'floors' and filled it with NaN.\n",
    "for s in df.loc[df['cabin'].notnull()]['floors'].index: # For each non-Nan value of cabin, we took the initial of the cabin number\n",
    "                                                        # and placed it in its corresponding row in the 'floors' column.\n",
    "    initial = str(df.loc[s,'cabin'])[0]\n",
    "    df.loc[s,'floors']=initial\n",
    "df[\"floors\"] = df[\"floors\"].astype('category')\n",
    "print(df['floors'].head())\n",
    "print(\"The possible floors in the ship are: \" + str(((df['floors'].dropna().unique()))) )   \n",
    "print(\"However, the number of missing data for 'cabin' attribute: \" + str(np.sum(pd.isnull(df['cabin']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*embarked*: As we previously mentioned, we expect that the values for *embarked* be S, Q, or C. We checked that here. We also found that there are only 2 missing values. We will, as done before, keep them untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The unique categories should be: {S,Q,C}\")\n",
    "print('The actual unique categories in the data given are: {}'.format(df['embarked'].dropna().unique()))\n",
    "print(\"Number of missing data for 'embarked' attribute: \" + str(np.sum(pd.isnull(df['embarked']))))\n",
    "df[\"embarked\"] = df[\"embarked\"].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2  <span style=\"color:Green\">Plot histograms for the travel class, embarkation port, sex and age attributes. For the latter one, use discrete decade intervals.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nice_bar_plot(data, ax, title='', y_axis=''):\n",
    "    ax.set_title(title , fontsize=16, fontweight='bold',color = 'darkgreen')\n",
    "    ax.set_xlabel(data.name); ax.set_ylabel(y_axis)\n",
    "    sns.barplot(x=data.value_counts().keys().values, y=data.value_counts().values,  ax=ax)\n",
    "    locs, labels = plt.xticks()\n",
    "    plt.setp(labels, rotation=90)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16,16))\n",
    "nice_bar_plot(df['sex'], axes[0,0],'Sex', 'Occurences Count')\n",
    "nice_bar_plot(np.floor(df['age'].divide(10))*10, axes[0,1],'Age', 'Occurences Count')\n",
    "nice_bar_plot(df['embarked'], axes[1,0],'Embarkation Port', 'Occurences Count')\n",
    "nice_bar_plot(df['pclass'], axes[1,1],'Travel Class', 'Occurences Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 <span style=\"color:Green\">Calculate the proportion of passengers by cabin floor. Present your results in a pie chart. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned, we have around 1000 missing values for both *cabin* and *floors* making the following proportions not very reliable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"It is worthy to note that around \" + str(np.sum(df['cabin'].isnull())/(len(df['cabin']))*100) + \"% of the data is missing.\")\n",
    "fig, axes = plt.subplots(1, 1, figsize=(16,16))\n",
    "ay = plt.subplot(1, 1, 1)\n",
    "labels = ['Floor: ' + s for s in df['floors'].value_counts().keys().values]\n",
    "explode = [0.1,0,0,0,0,0,0,0]\n",
    "plt.pie(df['floors'].value_counts().values, labels=labels,explode = explode,shadow=True, startangle = 140,autopct='%1.1f%%' )\n",
    "plt.title(\"Proportion of Passengers by Cabin Floor \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.4 <span style=\"color:Green\">For each travel class, calculate the proportion of the passengers that survived. Present your results in pie charts. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we don't have any missing data in either the \"Survived\" attribute or \"Pclass\" attribute, we can say that the data shown is a good representative. Additionally, we can see that the lower the passenger class (higher value in DataFrame), the lower the survival rate. This is logical as first class cabins might provide better escape plans. Also, it's probable that the priority has been given to first class passengers. To better visualize the survival rate - passenger class relation, we can take a look at the plot in axes[1,1]. The relation is almost perfectly linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16,16))\n",
    "df[df['pclass']==1]['survived'].value_counts().values\n",
    "i=1\n",
    "#for s in set(df['pclass'].value_counts().keys().values):\n",
    "#    print(df[df['pclass']==s]['survived'].value_counts())\n",
    "for s in set(df['pclass'].value_counts().keys().values):\n",
    "        print(\"In class: \" + str(s) + \" The percentage of people who survived (1) and didn't survive (0) is: \" + \n",
    "              \"\\n\" + str((df[df['pclass']==s]['survived'].value_counts(sort=False)).\n",
    "              divide((df[df['pclass']==s]['survived'].value_counts(sort=False).sum()))*100))\n",
    "        ay = plt.subplot(2, 2, i)\n",
    "        ay.set_title(\"Passenger Class: \" + str(s) , fontsize=12, fontweight='bold')\n",
    "        plt.pie(df[df['pclass']==s]['survived'].value_counts(sort=False).values,autopct='%1.1f%%', labels = [\"Didn't Survive\", \"Survived\"])\n",
    "        i = i+1\n",
    "survivearray = []       \n",
    "for s in range(1,4):\n",
    "    \n",
    "    a = df[df['pclass']==s]['survived'].value_counts(sort=False).divide((df[df['pclass']==s]['survived'].value_counts(sort=False).sum()))[1]\n",
    "    survivearray.append(a)\n",
    "   \n",
    "    \n",
    "print(survivearray)\n",
    "ax = plt.subplot(2, 2, 4)\n",
    "\n",
    "plt.plot([1.0,2.0,3], survivearray)\n",
    "ax.set_xlabel(\"Passenger Class\")\n",
    "ax.set_ylabel(\"Survival Rate Percentage\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5 <span style=\"color:Green\">Calculate the proportion of the passengers that survived by travel class and sex. Present your results in a single histogram. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"dark\")\n",
    "gplot = sns.factorplot(\"pclass\", \"survived\", \"sex\", data=df, size=8, kind=\"bar\", palette=\"hls\")\n",
    "gplot.set_ylabels(\"Proportion of Survived Passengers by \\\"Travel Class\\\" and \\\"Sex\\\"\")\n",
    "gplot.set_xlabels(\"Passenger Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.6 <span style=\"color:Green\">Create 2 equally populated age categories and calculate survival proportions by age category, travel class and sex. Present your results in a DataFrame with unique index.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we need to split the age attribute into two equally populated categories based on the median. To avoid further complexities related to running the median operation a second time on the categorical age, we create an additional attribute *agecat* containing the categorical age. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we notice that after splitting, the two categories are not exactly equally populated with 536 passengers in the first category and 510 in the second. This is due to the fact that there are many people with the age 28. This problem is unavoidable, given that you can't categorize people with the age 28 into two different categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['agecat'] = pd.cut(df.age, [0, df['age'].median(), df['age'].max()])\n",
    "df['agecat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df.groupby(['agecat', 'sex', 'pclass'])['survived'].agg(['count'])['count']\n",
    "t = df.groupby(['agecat', 'sex', 'pclass'])['survived'].sum() #Since survived gives a 1 and the opposite gives 0, we can use\n",
    "                                                           #sum() to calculate the number of people who survived, grouped by age,\n",
    "                                                           #sex, and passenger class.\n",
    "df_SurvivalRate = pd.DataFrame(t.divide(counts)*100)\n",
    "print(df_SurvivalRate.index.is_unique)\n",
    "df_SurvivalRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
